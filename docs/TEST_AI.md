# AI Chat Testing Guide

## ğŸš€ Quick Start

### 1. Start Local Server
```bash
python3 -m http.server 8080
```

Then open: http://localhost:8080/index.html

### 2. Test Scenarios

#### âœ… Test 1: Slash Commands (Should Work As Before)
- Type: `/help`
- Expected: Shows list of available commands
- Type: `/highlights`
- Expected: Shows research highlights

#### âœ… Test 2: First Chat Message (Consent Dialog)
- Type: `hello` (without slash)
- Expected: 
  - Shows consent dialog asking if you want to use AI
  - Two buttons: "Yes, let's chat!" and "No, I'll use commands"
  - Mentions model download size and suggests /help

#### âœ… Test 3: Accept AI (Background Loading)
- Click "Yes, let's chat!"
- Expected:
  - Shows "Loading AI model in background..." message
  - Model loading progress bar appears (overlay)
  - Processes your original "hello" message
  - If you type `/help` or other slash commands during load, they should work!
  - If you type another chat message during load, it says "model is still loading..."

#### âœ… Test 4: Decline AI (Command-Only Mode)
- Type: `hi` (without slash)
- Click "No, I'll use commands"
- Expected:
  - Shows helpful message about using /help
  - Future non-slash messages remind you to use commands
  - Slash commands work normally

#### âœ… Test 5: Easter Eggs (Instant Responses)
Try these for instant fun responses (no model loading needed):
- `sudo`
- `sudo make me a sandwich`
- `hack the planet`
- `hello world`
- `404`
- `rm -rf /`
- `konami code`
- `the answer`
- `do a barrel roll`

#### âœ… Test 6: Quick Responses (Instant)
Try these for instant responses:
- `hello` / `hi` / `hey`
- `tell me a joke`
- `who are you`
- `help`
- `thanks`
- `bye`

#### âœ… Test 7: AI Chat (After Model Loads)
Once model is loaded (you'll see "ğŸ‰ AI is ready!" message):
- Type: `what does cong research?`
- Expected: Streaming response about Cong's work in fuzzing and security
- Type: `tell me about fuzzing`
- Expected: Geeky explanation with personality

#### âœ… Test 8: Mixed Mode
After AI loads:
- Type: `/publications` (slash command)
- Expected: Shows publications
- Type: `explain the first paper` (chat)
- Expected: AI explains it
- Type: `/clear` (slash command)
- Expected: Clears console

## ğŸ® Key Features to Verify

### User Experience Flow
1. âœ… First non-slash input â†’ consent dialog
2. âœ… User accepts â†’ model loads in background
3. âœ… Slash commands work during loading
4. âœ… Progress bar shows loading status
5. âœ… Success message when ready
6. âœ… Streaming responses after ready

### Instant Responses (No Wait)
- âœ… Easter eggs work immediately
- âœ… Quick responses work immediately
- âœ… Slash commands always work

### Error Handling
- âœ… If model fails to load, shows clear error
- âœ… Suggests fallback to slash commands
- âœ… Mentions WebGPU requirement

## ğŸ› Common Issues

### Issue: "WebLLM library not loaded"
**Solution**: Check browser console, make sure webllm-loader.js is loading correctly

### Issue: Model download very slow
**Expected**: First download is ~300-500MB, can take 2-10 minutes depending on connection

### Issue: "Your browser doesn't support WebGPU"
**Solution**: 
- Use Chrome 113+ or Edge 113+
- Make sure hardware acceleration is enabled in browser settings

### Issue: Model loads but no response
**Check**: 
- Browser console for errors
- Make sure you typed a non-slash message after model loaded
- Try refreshing and testing again

## ğŸ“Š Browser Compatibility

âœ… **Supported:**
- Chrome 113+
- Edge 113+
- (WebGPU required)

âŒ **Not Supported:**
- Firefox (no WebGPU yet)
- Safari (limited WebGPU)
- Older Chrome/Edge versions

## ğŸ’¡ Pro Tips

1. **Test Easter Eggs First**: They're instant and don't require model loading
2. **Use /clear**: Clears console for clean testing
3. **Check Network Tab**: To see model download progress (first time only)
4. **Cached After First Load**: Second visit is much faster (model cached)
5. **Try During Loading**: Test slash commands while model loads to verify non-blocking

## ğŸ¨ Visual Checks

- âœ… User messages: blue-ish bubble
- âœ… AI messages: green-ish bubble  
- âœ… System messages: yellow-ish
- âœ… Error messages: red-ish
- âœ… Loading indicator: animated dots
- âœ… Progress bar: animated with shimmer effect
- âœ… ASCII art: monospace, properly formatted
- âœ… Buttons: green, hover effects

## ğŸš¨ What NOT to Do

- âŒ Don't push to upstream (as requested)
- âŒ Don't test on file:// protocol (won't work, needs http://)
- âŒ Don't expect instant AI responses on first message (model needs to load)
- âŒ Don't spam messages during loading (be patient)

## âœ¨ Expected Behavior Summary

```
User types non-slash for first time
  â†“
Consent dialog appears
  â†“
User clicks "Yes, let's chat!"
  â†“
Model starts loading in background (progress bar)
  â†“
MEANWHILE: Slash commands still work!
  â†“
Model finishes loading
  â†“
Success message: "ğŸ‰ AI is ready!"
  â†“
Now chat works with streaming responses
```

---

**Happy Testing! ğŸ‰**

If you find any issues, check the browser console for detailed error messages.
